{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画像認識練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11.0\n",
      "3.0.2\n"
     ]
    }
   ],
   "source": [
    "#basement module\n",
    "import chainer\n",
    "#import cupy\n",
    "#インストールが　macではできない。慎重に。\n",
    "import chainercv\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "import chainer.links as Link\n",
    "import chainer.functions as Function\n",
    "\n",
    "\n",
    "print(chainercv.__version__)\n",
    "print(matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必要となるデータセットは事前に用意してある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.asarray(Image.open(\"train/image/000.png\"))\n",
    "label = np.asarray(Image.open(\"train/label/000.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from chainer import datasets\n",
    "\n",
    "def create_dataset(img_filenames,label_filenames):\n",
    "    img = datasets.ImageDataset(img_filenames)\n",
    "    img = datasets.TransformDataset(img,lambda x: x/255.)\n",
    "    label = datasets.ImageDataset(label_filenames,dtype=np.int32)\n",
    "    dataset = datasets.TupleDataset(img,label)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets():\n",
    "    train_img_filenames = sorted(glob.glob(\"train/image/*.png\"))\n",
    "    train_label_filenames = sorted(glob.glob(\"train/label/*.png\"))\n",
    "    train = create_dataset(train_img_filenames,train_label_filenames)\n",
    "    \n",
    "    val_img_filenames = sorted(glob.glob(\"val/image/*.png\"))\n",
    "    val_label_filenames = sorted(glob.glob(\"val/label/*.png\"))\n",
    "    val = create_dataset(val_img_filenames,val_label_filenames)\n",
    "    return train,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,val = create_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(chainer.Chain):\n",
    "    def __init__(self,out_h,out_w):\n",
    "        super().__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = Link.Linear(None,100)\n",
    "            self.l2 = Link.Linear(100,100)\n",
    "            self.l3 = Link.Linear(100,out_h*out_w)\n",
    "        self.out_h = out_h\n",
    "        self.out_w = out_w\n",
    "        \n",
    "    def forward(self,x):\n",
    "        h = Function.relu(self.l1(x))\n",
    "        h = Function.relu(self.l2(h))\n",
    "        h = self.l3(h)\n",
    "        n = x.shape[0]\n",
    "        \n",
    "        return h.reshape((n,1,self.out_h,self.out_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer import iterators\n",
    "from chainer import training\n",
    "from chainer import optimizers\n",
    "from chainer.training import extensions\n",
    "\n",
    "def create_trainer(batchsize,train,val,stop,device=-1):\n",
    "    model = MultiLayerPerceptron(out_h=256,out_w=256)\n",
    "    train_model = Link.Classifier(model,lossfun=Function.sigmoid_cross_entropy,accfun=Function.binary_accuracy)\n",
    "    optimizer = optimizers.Adam()\n",
    "    optimizer.setup(train_model)\n",
    "    \n",
    "    train_iter = iterators.MultiprocessIterator(train,batchsize)\n",
    "    val_iter = iterators.MultiprocessIterator(val,batchsize,repeat=False,shuffle=False)\n",
    "    \n",
    "    updater = training.StandardUpdater(train_iter,optimizer,device=device)\n",
    "    trainer = training.trainer.Trainer(updater,stop)\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.9 s, sys: 8.63 s, total: 54.6 s\n",
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = create_trainer(64,train,val,(20,\"epoch\"),device=-1)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
